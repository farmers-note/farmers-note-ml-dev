{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s_yVF95R0yP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
        "# ==============================================================================\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install torchmetrics\n",
        "!pip install kagglehub\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset # ConcatDataset ì‚¬ìš©\n",
        "from torchvision import datasets, transforms\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from google.colab import drive # ëª¨ë¸ ì €ì¥ì„ ìœ„í•´ í•„ìš”\n",
        "\n",
        "# GPU ì„¤ì •\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ì‚¬ìš© ì¥ì¹˜: {device}\")\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "MODEL_NAME = 'efficientnet-b0'\n",
        "INPUT_SIZE = EfficientNet.get_image_size(MODEL_NAME)\n",
        "\n",
        "\n",
        
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •\n",
        "# ==============================================================================\n",
        "# ëª¨ë“  í•™ìŠµ ë° ê²€ì¦ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ì— í†µí•©\n",
        "\n",
        "path_rice = kagglehub.dataset_download(\"loki4514/rice-leaf-diseases-detection\") #ë²¼ ì§ˆë³‘ ë¶„ë¥˜ ì´ë¯¸ì§€ ë°ì´í„° api\n",
        "\n",
        "RICE_TRAIN_DIR = os.path.join(path_rice, 'Rice_Leaf_Diease', 'Rice_Leaf_Diease', 'train')\n",
        "RICE_VAL_DIR = os.path.join(path_rice, 'Rice_Leaf_Diease', 'Rice_Leaf_Diease', 'test')\n",
        "\n",
        "\n",
        "ALL_TRAIN_DIRS = [RICE_TRAIN_DIR, STRAWBERRY_TRAIN_DIR, POTATO_TRAIN_DIR]\n",
        "ALL_TEST_DIRS = [RICE_VAL_DIR, STRAWBERRY_VAL_DIR, POTATO_VAL_DIR]\n",
        "\n",
        "\n",
        "print(f\"ìµœì¢… TRAIN ê²½ë¡œ: {TRAIN_DIR}\")\n",
        "print(f\"ìµœì¢… VAL ê²½ë¡œ: {VAL_DIR}\")"
      ],
      "metadata": {
        "id": "SS_T7pFdTfdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. ë°ì´í„° ë¡œë“œ ë° ConcatDataset í†µí•©\n",
        "# ==============================================================================\n",
        "\n",
        "# ë°ì´í„° ë³€í™˜ (ëª¨ë“  ì‘ë¬¼ì— ë™ì¼í•˜ê²Œ ì ìš©)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(INPUT_SIZE), transforms.RandomResizedCrop(INPUT_SIZE),\n",
        "        transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(INPUT_SIZE), transforms.CenterCrop(INPUT_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "train_datasets = []\n",
        "test_datasets = []\n",
        "all_class_names = []\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„° í†µí•©\n",
        "for dir_path in ALL_TRAIN_DIRS:\n",
        "    if os.path.exists(dir_path):\n",
        "        current_dataset = datasets.ImageFolder(dir_path, data_transforms['train'])\n",
        "        train_datasets.append(current_dataset)\n",
        "        all_class_names.extend(current_dataset.classes)\n",
        "        print(f\"âœ… {os.path.basename(dir_path)} ë¡œë“œ ì™„ë£Œ. í´ë˜ìŠ¤: {current_dataset.classes}\")\n",
        "    else:\n",
        "        print(f\"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (Train): {dir_path}. ì´ ë°ì´í„°ì…‹ì€ ì œì™¸ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„° í†µí•©\n",
        "for dir_path in ALL_TEST_DIRS:\n",
        "    if os.path.exists(dir_path):\n",
        "        current_dataset = datasets.ImageFolder(dir_path, data_transforms['test'])\n",
        "        test_datasets.append(current_dataset)\n",
        "    # âš ï¸ ê²€ì¦ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ì´ë¦„ì€ í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "# ğŸ’¡ ëª¨ë“  ì‘ë¬¼ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ í° ë°ì´í„°ì…‹ìœ¼ë¡œ í•©ì¹˜ê¸°\n",
        "combined_train_dataset = ConcatDataset(train_datasets)\n",
        "combined_test_dataset = ConcatDataset(test_datasets)\n",
        "\n",
        "# ìµœì¢… í´ë˜ìŠ¤ ëª©ë¡ (ì¤‘ë³µ ì œê±°)\n",
        "final_class_names = sorted(list(set(all_class_names)))\n",
        "NUM_CLASSES = len(final_class_names)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(combined_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4),\n",
        "    'test': DataLoader(combined_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(combined_train_dataset), 'test': len(combined_test_dataset)}\n",
        "\n",
        "print(f\"\\nâœ… í†µí•© í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {dataset_sizes['train']}\")\n",
        "print(f\"âœ… í†µí•© ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {dataset_sizes['test']}\")\n",
        "print(f\"âœ… í†µí•©ëœ ì „ì²´ í´ë˜ìŠ¤ ê°œìˆ˜: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "-oTZRBX3TmQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. ëª¨ë¸ ì„¤ì • (EfficientNet-B0 ì „ì´ í•™ìŠµ)\n",
        "# ==============================================================================\n",
        "\n",
        "model_ft = EfficientNet.from_pretrained(MODEL_NAME, num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)"
      ],
      "metadata": {
        "id": "15Nczz5VdWMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (test í‚¤ ì‚¬ìš©ìœ¼ë¡œ ìˆ˜ì •)\n",
        "# ==============================================================================\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}'); print('-' * 10)\n",
        "\n",
        "        # âš ï¸ 'val' ëŒ€ì‹  'test' í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train': model.train()\n",
        "            else: model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train': scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            # âš ï¸ ì¶œë ¥ ë©”ì‹œì§€ë„ 'test'ë¡œ ìˆ˜ì •\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # âš ï¸ 'test' ë‹¨ê³„ì˜ ì •í™•ë„ê°€ ìµœê³  ê¸°ë¡ì¼ ë•Œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                print(f'-> New best model found! Saving weights with Acc: {best_acc:.4f}')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\ní•™ìŠµ ì™„ë£Œ! ì´ ì‹œê°„: {time_elapsed // 60:.0f}ë¶„ {time_elapsed % 60:.0f}ì´ˆ')\n",
        "    print(f'ìµœê³  ê²€ì¦ ì •í™•ë„: {best_acc:.4f}')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nztWbuEedcS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 6. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ë° ì €ì¥\n",
        "# ==============================================================================\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=NUM_EPOCHS)\n"
      ],
      "metadata": {
        "id": "qkqV3csHdx7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í˜„ì¬ í• ë‹¹ëœ GPU ì •ë³´ í™•ì¸\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-VSEopjJluwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ (ì €ì¥ì„ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤)\n",
        "drive.mount('/content/drive')\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/efficientnet_rice_disease_best_model.pth'\n",
        "\n",
        "# ëª¨ë¸ ì €ì¥\n",
        "torch.save(model_ft.state_dict(), MODEL_SAVE_PATH)\n",
        "print(f\"\\nâœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ {MODEL_SAVE_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "_6HDp--VlfXu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
